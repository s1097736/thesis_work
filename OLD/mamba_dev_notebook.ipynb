{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat\n",
    "import pickle\n",
    "from tqdm import tqdm \n",
    "import math\n",
    "from transformers import GPT2Tokenizer\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "@dataclass\n",
    "class LMTrainingArgs:\n",
    "    # NB this is not the actual learning rate, see below! \n",
    "    gpt_3_peak_lr: float # GPT3 spec, copy from table depending on size\n",
    "    warmup_epochs: int \n",
    "    n_epochs: int\n",
    "\n",
    "    # default arguments, specified according to Mamba paper. This is the \n",
    "    # default for language modeling, for artificial tasks use the other\n",
    "    # args class!\n",
    "\n",
    "    min_lr: float = 1e-5\n",
    "    weight_decay: float = 0.1\n",
    "    gradient_clip: float = 1.0\n",
    "    adam_beta: tuple = (0.9, 0.95)\n",
    "    adam_epsilon: float = 1e-8\n",
    "    optimizer: str = \"AdamW\" # TODO: implement ability to use Adam, just because\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.peak_lr: float = 5*self.gpt_3_peak_lr # the actual lr in the training recipe\n",
    "        self.schedule_fn: Callable[[int], float] = self.lm_learning_schedule\n",
    "\n",
    "        assert self.warmup_epochs < self.n_epochs, \"Warmup epochs > total epochs\"\n",
    "        assert self.optimizer == \"AdamW\" or self.optimizer == \"Adam\", 'Invalid optimizer'\n",
    "        \n",
    "    def lm_learning_schedule(self, epoch):\n",
    "        # a cosine decay with a minimum value, with a linear warm-up\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return float(epoch+1) / float(max(1, self.warmup_epochs))\n",
    "        else:\n",
    "            # calculate amount of decay progress\n",
    "            progress = float(epoch - self.warmup_epochs + 1) / \\\n",
    "                            float(max(1, self.n_epochs - self.warmup_epochs))\n",
    "            # shift cosine function up, rescale, and compute the appropriate amount\n",
    "            # of decay\n",
    "            cosine_decay = 0.5 * (1+ math.cos(math.pi * progress))\n",
    "            # rescale the function again so that it doesn't go below the minimum\n",
    "            # value\n",
    "            return cosine_decay * (1 - self.min_lr / self.peak_lr) + self.min_lr / self.peak_lr\n",
    "    \n",
    "    def show_lr_schedule(self):\n",
    "        # visualization of the learning rate schedule given the specified\n",
    "        # training protocol\n",
    "        epochs = np.arange(0, self.n_epochs)\n",
    "        lr = np.zeros(len(epochs))\n",
    "        for e in epochs:\n",
    "            lr[e] = self.peak_lr*self.lm_learning_schedule(e)\n",
    "\n",
    "        min_lr = np.min(lr[self.warmup_epochs+1:])\n",
    "        max_lr = np.max(lr)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlim([0,self.n_epochs-1])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Learning rate\")\n",
    "        plt.title(f\"Max = {max_lr}, \\nMin = {min_lr:.2e}\")\n",
    "        plt.show()\n",
    "\n",
    "@dataclass\n",
    "class MambaArgs:\n",
    "    N: int \n",
    "    D: int\n",
    "    n_layers: int\n",
    "    vocab_size: int\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    expansion_factor: int = 2\n",
    "    conv_1d_size: int = 4\n",
    "    conv_bias: bool = True\n",
    "    general_bias: bool = False # applies to the input and output projections\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.D_inner = int(self.expansion_factor * self.D)\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    ''' Full Mamba architecture '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(Mamba, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.D)\n",
    "\n",
    "        self.layers = nn.ModuleList([ResidualMambaBlock(args) \n",
    "                                     for _ in range(args.n_layers)])\n",
    "        self.norm_f = RMSNorm(args.D)\n",
    "\n",
    "        self.logits = nn.Linear(args.D, args.vocab_size, bias=False)\n",
    "        self.logits.weight = self.embedding.weight # weight tying! \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.logits(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class ResidualMambaBlock(nn.Module):\n",
    "    ''' Wraps the standard Mamba block with RMS normalization and residual\n",
    "        connections (used everywhere)'''\n",
    "    \n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(ResidualMambaBlock, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.block = MambaBlock(args)\n",
    "        self.rms = RMSNorm(args.D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(self.rms(x)) + x\n",
    "    \n",
    "class MambaBlock(nn.Module):\n",
    "    ''' Standard Mamba block as illustrated in the paper '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(MambaBlock, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        # takes care of both of the upscale projections, factor of 2!\n",
    "        self.in_proj = nn.Linear(args.D, 2*args.D_inner, bias=args.general_bias)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.D_inner,\n",
    "            out_channels=args.D_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.conv_1d_size,\n",
    "            groups=args.D_inner,\n",
    "            padding=args.conv_1d_size - 1,\n",
    "        )    \n",
    "        self.s6_block = S6Block(args)    \n",
    "\n",
    "        self.out_proj = nn.Linear(args.D_inner, args.D, bias=args.general_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, _ = x.shape # used to avoid specifying these in args\n",
    "        x = self.in_proj(x)\n",
    "        # split the input into the two paths\n",
    "        (x, res) = x.split(\n",
    "            split_size=[self.args.D_inner, self.args.D_inner], dim=-1)\n",
    "\n",
    "        # input of shape (B,L,D), dimensions need switching for convolution\n",
    "        x = torch.transpose(x, 1,2)\n",
    "        x = self.conv1d(x)[:,:,:l] # the limit is needed because of the padding\n",
    "        x = torch.transpose(x, 1,2)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        x = self.s6_block(x)\n",
    "        x = x * F.silu(res)\n",
    "\n",
    "        y = self.out_proj(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "class S6Block(nn.Module):\n",
    "    ''' Inner SSM block '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "\n",
    "        super(S6Block, self).__init__()\n",
    "        self.args = args \n",
    "\n",
    "        def s4d_real():\n",
    "            # initialization for A used in the paper. Other complex-valued \n",
    "            # initializations also possible\n",
    "\n",
    "            # compute one diagonal, then broadcast across D dimensions. NB\n",
    "            # that this output is missing a minus sign; we update A in log space,\n",
    "            # so this minus is only added in during the forward pass\n",
    "            A = torch.arange(0,args.N)+1\n",
    "\n",
    "            return A.unsqueeze(0).repeat(args.D_inner,1).float()\n",
    "\n",
    "        def get_delta_bias():\n",
    "            # sample from a uniform distribution bounded within these values,\n",
    "            # then pass through an inverse softplus\n",
    "            a = 0.001\n",
    "            b = 0.1\n",
    "            sample = (b-a)* torch.rand(1) + a\n",
    "            # no built-in pytorch version of inverse softplus... numerical issues?\n",
    "            return torch.log(torch.exp(sample-1))\n",
    "\n",
    "        self.log_minus_A = nn.Parameter(torch.log(s4d_real()))\n",
    "        \n",
    "        # these are strictly linear projections, no biases used ever.\n",
    "        # delta uses one, which we manually add later. \n",
    "        self.to_BCdelta = nn.Linear(args.D_inner, 2*args.N+1, bias=False)\n",
    "\n",
    "        # explicit delta bias term\n",
    "        self.delta_bias = nn.Parameter(get_delta_bias()) #TODO:  Check for training\n",
    "        \n",
    "    def discretize(self, delta, B):\n",
    "\n",
    "        # ZOH discretization. Official implementation approximates B_bar with\n",
    "        # Euler step instead\n",
    "        delta_A = torch.einsum('bld,dn->bldn', delta, -torch.exp(self.log_minus_A))\n",
    "        A_bar = torch.exp(delta_A)\n",
    "        delta_B = torch.einsum('bld,bln->bldn', delta, B)\n",
    "        # diagonal matrices, so 1/A is the inverse, subtracting 1 instead \n",
    "        # of the identity matrix, and directly multiplying elementwise for the \n",
    "        # first multiplication (second is defined elementwise anyway)\n",
    "        B_bar = 1/(delta_A) * (A_bar - 1) * delta_B\n",
    "\n",
    "        return A_bar, B_bar\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, _ = x.shape \n",
    "        # generate all projected parameters and split them up\n",
    "        BCdelta = self.to_BCdelta(x)\n",
    "        # delta: (B, L, 1). B, C: (B, L, N)\n",
    "        (B, C, delta) = BCdelta.split(\n",
    "            split_size=[self.args.N, self.args.N, 1], dim=-1)\n",
    "\n",
    "        # broadcasting for delta and computing final parameters\n",
    "        delta = delta.repeat(1,1,self.args.D_inner) # (B,L,D)\n",
    "        delta += self.delta_bias\n",
    "        delta = F.softplus(delta)\n",
    "\n",
    "        # discretization\n",
    "        A_bar, B_bar = self.discretize(delta, B) # (B, L, D, N)\n",
    "        \n",
    "        # input transformation is parallelizable\n",
    "        input_transform = B_bar * x.unsqueeze(-1) # (B, L, D, N)\n",
    "        \n",
    "        # scan through each individual token to compute hidden states\n",
    "        hidden_states = torch.zeros(\n",
    "            b, l+1, self.args.D_inner, self.args.N).to(self.args.device)\n",
    "        \n",
    "        for i in range(0,l):\n",
    "            # because A is represented only through diagonal, Ah_t-1 is \n",
    "            # equivalent to taking the elementwise product of the diagonal\n",
    "            # and the hidden state\n",
    "            hidden_states[:,i+1,:,:] = A_bar[:,i,:,:]*hidden_states[:,i,:,:].clone() + \\\n",
    "                input_transform[:,i,:,:] # (B,D,N)\n",
    "        \n",
    "        # compute outputs in parallel\n",
    "        outputs = torch.einsum('bln,bldn->bld', C, hidden_states[:,1:,:,:])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    ''' Simple implementation of RMSNorm. Default implementation is bugged\n",
    "        in this version of PyTorch, don't want to mess with version updating '''\n",
    "    def __init__(self,\n",
    "                 D: int,\n",
    "                 eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(D))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "with open(\"kaggle_song_lyrics_dataset/kaggle_song_lyrics_dataset.pkl\", \"rb\") as f:\n",
    "    seqs = pickle.load(f)\n",
    "\n",
    "print(type(seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPT2 tokenizer tokenizer to create a tokenized version of the kaggle \n",
    "# song lyrics dataset. Simply training to predict the next word here. \n",
    "\n",
    "L = 32\n",
    "B = 8\n",
    "D = 16\n",
    "N = 8\n",
    "\n",
    "vocab_size = 5000\n",
    "device = 'mps' # hehe \n",
    "\n",
    "mamba_args = MambaArgs(N, D, n_layers=5, vocab_size=vocab_size, device=device)\n",
    "model = Mamba(mamba_args).to(mamba_args.device)\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", force_download=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2tokenizer\")\n",
    "\n",
    "with open(\"kaggle_song_lyrics_dataset/kaggle_song_lyrics_dataset.pkl\", \"rb\") as f:\n",
    "    seqs = pickle.load(f)\n",
    "\n",
    "# shorten the dataset...\n",
    "seqs = seqs[1:10000]\n",
    "\n",
    "# get tokens and corresponding IDs\n",
    "tokens = [tokenizer.tokenize(word) for word in seqs]\n",
    "token_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "# flattening them all in one list\n",
    "token_ids = [item for sublist in token_ids for item in sublist] \n",
    "\n",
    "# limit all tokens to the top n most common. Replace the less common occurrences\n",
    "# with unk token \n",
    "filtered_ids = []\n",
    "unk_id = tokenizer.get_vocab()['unk']\n",
    "for token_id in token_ids:\n",
    "\n",
    "    # if given vocab size is not large enough to include the unk token itself, \n",
    "    # the vocab size must be reduced by 1 to fit this in\n",
    "    if unk_id > mamba_args.vocab_size-1:\n",
    "        vocab_limit = mamba_args.vocab_size-2\n",
    "    else:\n",
    "        vocab_limit = mamba_args.vocab_size-1\n",
    "\n",
    "    # filter the text to only include top n most common words\n",
    "    if token_id > vocab_limit and token_id != unk_id:\n",
    "        filtered_ids.append(unk_id)\n",
    "    else:\n",
    "        filtered_ids.append(token_id)\n",
    "\n",
    "# put the tokenized sequences in datasets + dataloaders\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, device, seq_size, seqs):\n",
    "        super(SeqDataset, self).__init__()\n",
    "        self.device   = device\n",
    "        self.seq_size = seq_size\n",
    "        self.seqs     = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs) - self.seq_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_seq     = torch.tensor(self.seqs[idx    :idx + self.seq_size    ], dtype=torch.long, device=self.device)\n",
    "        target_seq = torch.tensor(self.seqs[idx + 1:idx + self.seq_size + 1], dtype=torch.long, device=self.device)\n",
    "        return in_seq, target_seq\n",
    "\n",
    "# 80 10 10 split between the 3 datasets\n",
    "train_set = SeqDataset(mamba_args.device, L, filtered_ids[:int(0.8 * len(filtered_ids))])\n",
    "val_set   = SeqDataset(mamba_args.device, L, filtered_ids[ int(0.8 * len(filtered_ids)) + 1:int(0.9 * len(filtered_ids))])\n",
    "test_set  = SeqDataset(mamba_args.device, L, filtered_ids[ int(0.9 * len(filtered_ids)) + 1:])\n",
    "\n",
    "train_loader = DataLoader(train_set, B, shuffle=True)\n",
    "val_loader   = DataLoader(val_set  , B, shuffle=False)\n",
    "test_loader  = DataLoader(test_set , B, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/1200 [00:11<46:18,  2.32s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# gradient clipping\u001b[39;00m\n\u001b[1;32m     39\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), train_args\u001b[38;5;241m.\u001b[39mgradient_clip)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    523\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m _engine_run_backward(\n\u001b[1;32m    290\u001b[0m     tensors,\n\u001b[1;32m    291\u001b[0m     grad_tensors_,\n\u001b[1;32m    292\u001b[0m     retain_graph,\n\u001b[1;32m    293\u001b[0m     create_graph,\n\u001b[1;32m    294\u001b[0m     inputs,\n\u001b[1;32m    295\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    296\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    297\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training! \n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "train_args = LMTrainingArgs(gpt_3_peak_lr=1.5e-3, warmup_epochs=10, n_epochs=100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if train_args.optimizer == \"AdamW\":\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                                lr=train_args.peak_lr, \n",
    "                                betas=train_args.adam_beta,\n",
    "                                eps=train_args.adam_epsilon,\n",
    "                                weight_decay=train_args.weight_decay)\n",
    "    \n",
    "elif train_args.optimizer == \"Adam\": # used in synthetic tasks\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                                lr=train_args.peak_lr, \n",
    "                                betas=train_args.adam_beta,\n",
    "                                eps=train_args.adam_epsilon)    \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=train_args.schedule_fn)\n",
    "\n",
    "# train_args.show_lr_schedule()\n",
    "\n",
    "for epoch in range(train_args.n_epochs):\n",
    "    print(f\"Epoch: {epoch + 1}/{train_args.n_epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for in_seq, target_seq in tqdm(train_loader):\n",
    "        out_seq = model(in_seq)\n",
    "        loss = criterion(out_seq.view(-1, mamba_args.vocab_size), target_seq.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), train_args.gradient_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_perplexity = math.exp(train_loss)\n",
    "    print(f\"Train loss: {train_loss:.4f}, Train perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for in_seq, target_seq in val_loader:\n",
    "            out_seq, _ = model(in_seq)\n",
    "            loss = criterion(out_seq.view(-1, mamba_args.vocab_size), target_seq.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_perplexity = math.exp(val_loss)\n",
    "    print(f\"Val loss: {val_loss:.4f}, Val perplexity: {val_perplexity:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
