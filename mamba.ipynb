{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import jax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat\n",
    "import pickle\n",
    "import collections\n",
    "from tqdm import tqdm \n",
    "import math\n",
    "from transformers import GPT2Tokenizer\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "@dataclass\n",
    "class LMTrainingArgs:\n",
    "    # NB this is not the actual learning rate, see below! \n",
    "    gpt_3_peak_lr: float # GPT3 spec, copy from table depending on size\n",
    "    warmup_epochs: int \n",
    "    n_epochs: int\n",
    "\n",
    "    # default arguments, specified according to Mamba paper. This is the \n",
    "    # default for language modeling, for artificial tasks use the other\n",
    "    # args class!\n",
    "\n",
    "    min_lr: float = 1e-5\n",
    "    weight_decay: float = 0.1\n",
    "    gradient_clip: float = 1.0\n",
    "    adam_beta: tuple = (0.9, 0.95)\n",
    "    adam_epsilon: float = 1e-8\n",
    "    optimizer: str = \"AdamW\" # TODO: implement ability to use Adam, just because\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.peak_lr: float = 5*self.gpt_3_peak_lr # the actual lr in the training recipe\n",
    "        self.schedule_fn: Callable[[int], float] = self.lm_learning_schedule\n",
    "\n",
    "        assert self.warmup_epochs < self.n_epochs, \"Warmup epochs > total epochs\"\n",
    "        assert self.optimizer == \"AdamW\" or self.optimizer == \"Adam\", 'Invalid optimizer'\n",
    "        \n",
    "    def lm_learning_schedule(self, epoch):\n",
    "        # a cosine decay with a minimum value, with a linear warm-up\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return float(epoch+1) / float(max(1, self.warmup_epochs))\n",
    "        else:\n",
    "            # calculate amount of decay progress\n",
    "            progress = float(epoch - self.warmup_epochs + 1) / \\\n",
    "                            float(max(1, self.n_epochs - self.warmup_epochs))\n",
    "            # shift cosine function up, rescale, and compute the appropriate amount\n",
    "            # of decay\n",
    "            cosine_decay = 0.5 * (1+ math.cos(math.pi * progress))\n",
    "            # rescale the function again so that it doesn't go below the minimum\n",
    "            # value\n",
    "            return cosine_decay * (1 - self.min_lr / self.peak_lr) + self.min_lr / self.peak_lr\n",
    "    \n",
    "    def show_lr_schedule(self):\n",
    "        # visualization of the learning rate schedule given the specified\n",
    "        # training protocol\n",
    "        epochs = np.arange(0, self.n_epochs)\n",
    "        lr = np.zeros(len(epochs))\n",
    "        for e in epochs:\n",
    "            lr[e] = self.peak_lr*self.lm_learning_schedule(e)\n",
    "\n",
    "        min_lr = np.min(lr[self.warmup_epochs+1:])\n",
    "        max_lr = np.max(lr)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlim([0,self.n_epochs-1])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Learning rate\")\n",
    "        plt.title(f\"Max = {max_lr}, \\nMin = {min_lr:.2e}\")\n",
    "        plt.show()\n",
    "\n",
    "@dataclass\n",
    "class MambaArgs:\n",
    "    N: int \n",
    "    D: int\n",
    "    n_layers: int\n",
    "    vocab_size: int\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    expansion_factor: int = 2\n",
    "    conv_1d_size: int = 4\n",
    "    conv_bias: bool = True\n",
    "    general_bias: bool = False # applies to the input and output projections\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.D_inner = int(self.expansion_factor * self.D)\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    ''' Full Mamba architecture '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(Mamba, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.D)\n",
    "\n",
    "        self.layers = nn.ModuleList([ResidualMambaBlock(args) \n",
    "                                     for _ in range(args.n_layers)])\n",
    "        self.norm_f = RMSNorm(args.D)\n",
    "\n",
    "        self.logits = nn.Linear(args.D, args.vocab_size, bias=False)\n",
    "        self.logits.weight = self.embedding.weight # weight tying! \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.logits(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class ResidualMambaBlock(nn.Module):\n",
    "    ''' Wraps the standard Mamba block with RMS normalization and residual\n",
    "        connections (used everywhere)'''\n",
    "    \n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(ResidualMambaBlock, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.block = MambaBlock(args)\n",
    "        self.rms = RMSNorm(args.D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(self.rms(x)) + x\n",
    "    \n",
    "class MambaBlock(nn.Module):\n",
    "    ''' Standard Mamba block as illustrated in the paper '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(MambaBlock, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        # takes care of both of the upscale projections, factor of 2!\n",
    "        self.in_proj = nn.Linear(args.D, 2*args.D_inner, bias=args.general_bias)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.D_inner,\n",
    "            out_channels=args.D_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.conv_1d_size,\n",
    "            groups=args.D_inner,\n",
    "            padding=args.conv_1d_size - 1,\n",
    "        )    \n",
    "        self.s6_block = S6Block(args)    \n",
    "\n",
    "        self.out_proj = nn.Linear(args.D_inner, args.D, bias=args.general_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, _ = x.shape # used to avoid specifying these in args\n",
    "        x = self.in_proj(x)\n",
    "        # split the input into the two paths\n",
    "        (x, res) = x.split(\n",
    "            split_size=[self.args.D_inner, self.args.D_inner], dim=-1)\n",
    "\n",
    "        # input of shape (B,L,D), dimensions need switching for convolution\n",
    "        x = torch.transpose(x, 1,2)\n",
    "        x = self.conv1d(x)[:,:,:l] # the limit is needed because of the padding\n",
    "        x = torch.transpose(x, 1,2)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        x = self.s6_block(x)\n",
    "        x = x * F.silu(res)\n",
    "\n",
    "        y = self.out_proj(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "class S6Block(nn.Module):\n",
    "    ''' Inner SSM block '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "\n",
    "        super(S6Block, self).__init__()\n",
    "        self.args = args \n",
    "\n",
    "        def s4d_real():\n",
    "            # initialization for A used in the paper. Other complex-valued \n",
    "            # initializations also possible\n",
    "\n",
    "            # compute one diagonal, then broadcast across D dimensions. NB\n",
    "            # that this output is missing a minus sign; we update A in log space,\n",
    "            # so this minus is only added in during the forward pass\n",
    "            A = torch.arange(0,args.N)+1\n",
    "\n",
    "            return A.unsqueeze(0).repeat(args.D_inner,1).float()\n",
    "\n",
    "        def get_delta_bias():\n",
    "            # sample from a uniform distribution bounded within these values,\n",
    "            # then pass through an inverse softplus\n",
    "            a = 0.001\n",
    "            b = 0.1\n",
    "            sample = (b-a)* torch.rand(1) + a\n",
    "            # no built-in pytorch version of inverse softplus... numerical issues?\n",
    "            return torch.log(torch.exp(sample-1))\n",
    "\n",
    "        self.log_minus_A = nn.Parameter(torch.log(s4d_real()))\n",
    "        \n",
    "        # these are strictly linear projections, no biases used ever.\n",
    "        # delta uses one, which we manually add later. \n",
    "        self.to_BCdelta = nn.Linear(args.D_inner, 2*args.N+1, bias=False)\n",
    "\n",
    "        # explicit delta bias term\n",
    "        self.delta_bias = nn.Parameter(get_delta_bias()) #TODO:  Check for training\n",
    "        \n",
    "    def discretize(self, delta, B):\n",
    "\n",
    "        # ZOH discretization. Official implementation approximates B_bar with\n",
    "        # Euler step instead\n",
    "        delta_A = torch.einsum('bld,dn->bldn', delta, -torch.exp(self.log_minus_A))\n",
    "        A_bar = torch.exp(delta_A)\n",
    "        delta_B = torch.einsum('bld,bln->bldn', delta, B)\n",
    "        # diagonal matrices, so 1/A is the inverse, subtracting 1 instead \n",
    "        # of the identity matrix, and directly multiplying elementwise for the \n",
    "        # first multiplication (second is defined elementwise anyway)\n",
    "        B_bar = 1/(delta_A) * (A_bar - 1) * delta_B\n",
    "\n",
    "        return A_bar, B_bar\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, _ = x.shape \n",
    "        # generate all projected parameters and split them up\n",
    "        BCdelta = self.to_BCdelta(x)\n",
    "        # delta: (B, L, 1). B, C: (B, L, N)\n",
    "        (B, C, delta) = BCdelta.split(\n",
    "            split_size=[self.args.N, self.args.N, 1], dim=-1)\n",
    "\n",
    "        # broadcasting for delta and computing final parameters\n",
    "        delta = delta.repeat(1,1,self.args.D_inner) # (B,L,D)\n",
    "        delta += self.delta_bias\n",
    "        delta = F.softplus(delta)\n",
    "\n",
    "        # discretization\n",
    "        A_bar, B_bar = self.discretize(delta, B) # (B, L, D, N)\n",
    "        \n",
    "        # input transformation is parallelizable\n",
    "        input_transform = B_bar * x.unsqueeze(-1) # (B, L, D, N)\n",
    "        \n",
    "        # scan through each individual token to compute hidden states\n",
    "        hidden_states = torch.zeros(\n",
    "            b, l+1, self.args.D_inner, self.args.N).to(self.args.device)\n",
    "        \n",
    "        for i in range(0,l):\n",
    "            # because A is represented only through diagonal, Ah_t-1 is \n",
    "            # equivalent to taking the elementwise product of the diagonal\n",
    "            # and the hidden state\n",
    "            hidden_states[:,i+1,:,:] = A_bar[:,i,:,:]*hidden_states[:,i,:,:].clone() + \\\n",
    "                input_transform[:,i,:,:] # (B,D,N)\n",
    "        \n",
    "        # compute outputs in parallel\n",
    "        outputs = torch.einsum('bln,bldn->bld', C, hidden_states[:,1:,:,:])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    ''' Simple implementation of RMSNorm. Default implementation is bugged\n",
    "        in this version of PyTorch, don't want to mess with version updating '''\n",
    "    def __init__(self,\n",
    "                 D: int,\n",
    "                 eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(D))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPT3 tokenizer tokenizer to create a tokenized version of the kaggle \n",
    "# song lyrics dataset. Simply training to predict the next word here. \n",
    "\n",
    "L = 32\n",
    "B = 8\n",
    "D = 16\n",
    "N = 8\n",
    "\n",
    "vocab_size = 5000\n",
    "device = 'mps' # hehe \n",
    "\n",
    "mamba_args = MambaArgs(N, D, n_layers=5, vocab_size=vocab_size, device=device)\n",
    "model = Mamba(mamba_args).to(mamba_args.device)\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", force_download=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"model\")\n",
    "\n",
    "with open(\"kaggle_song_lyrics_dataset/kaggle_song_lyrics_dataset.pkl\", \"rb\") as f:\n",
    "    seqs = pickle.load(f)\n",
    "\n",
    "# shorten the dataset...\n",
    "seqs = seqs[1:10000]\n",
    "\n",
    "# get tokens and corresponding IDs\n",
    "tokens = [tokenizer.tokenize(word) for word in seqs]\n",
    "token_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "# flattening them all in one list\n",
    "token_ids = [item for sublist in token_ids for item in sublist] \n",
    "\n",
    "# limit all tokens to the top n most common. Replace the less common occurrences\n",
    "# with unk token \n",
    "filtered_ids = []\n",
    "unk_id = tokenizer.get_vocab()['unk']\n",
    "for token_id in token_ids:\n",
    "\n",
    "    # if given vocab size is not large enough to include the unk token itself, \n",
    "    # the vocab size must be reduced by 1 to fit this in\n",
    "    if unk_id > mamba_args.vocab_size-1:\n",
    "        vocab_limit = mamba_args.vocab_size-2\n",
    "    else:\n",
    "        vocab_limit = mamba_args.vocab_size-1\n",
    "\n",
    "    # filter the text to only include top n most common words\n",
    "    if token_id > vocab_limit and token_id != unk_id:\n",
    "        filtered_ids.append(unk_id)\n",
    "    else:\n",
    "        filtered_ids.append(token_id)\n",
    "\n",
    "# put the tokenized sequences in datasets + dataloaders\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, device, seq_size, seqs):\n",
    "        super(SeqDataset, self).__init__()\n",
    "        self.device   = device\n",
    "        self.seq_size = seq_size\n",
    "        self.seqs     = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs) - self.seq_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_seq     = torch.tensor(self.seqs[idx    :idx + self.seq_size    ], dtype=torch.long, device=self.device)\n",
    "        target_seq = torch.tensor(self.seqs[idx + 1:idx + self.seq_size + 1], dtype=torch.long, device=self.device)\n",
    "        return in_seq, target_seq\n",
    "\n",
    "# 90 10 10 split between the 3 datasets\n",
    "train_set = SeqDataset(mamba_args.device, L, filtered_ids[:int(0.8 * len(filtered_ids))])\n",
    "val_set   = SeqDataset(mamba_args.device, L, filtered_ids[ int(0.8 * len(filtered_ids)) + 1:int(0.9 * len(filtered_ids))])\n",
    "test_set  = SeqDataset(mamba_args.device, L, filtered_ids[ int(0.9 * len(filtered_ids)) + 1:])\n",
    "\n",
    "train_loader = DataLoader(train_set, B, shuffle=True)\n",
    "val_loader   = DataLoader(val_set  , B, shuffle=False)\n",
    "test_loader  = DataLoader(test_set , B, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1200 [00:13<37:45,  1.90s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[219], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m in_seq, target_seq \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m---> 32\u001b[0m     out_seq \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(out_seq\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, mamba_args\u001b[38;5;241m.\u001b[39mvocab_size), target_seq\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     34\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[217], line 98\u001b[0m, in \u001b[0;36mMamba.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 98\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_f(x)\n\u001b[1;32m    101\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[217], line 117\u001b[0m, in \u001b[0;36mResidualMambaBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[217], line 153\u001b[0m, in \u001b[0;36mMambaBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtranspose(x, \u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    152\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(x)\n\u001b[0;32m--> 153\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ms6_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m*\u001b[39m F\u001b[38;5;241m.\u001b[39msilu(res)\n\u001b[1;32m    156\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_proj(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[217], line 237\u001b[0m, in \u001b[0;36mS6Block.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    230\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\n\u001b[1;32m    231\u001b[0m     b, l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mD_inner, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mN)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,l):\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# because A is represented only through diagonal, Ah_t-1 is \u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# equivalent to taking the elementwise product of the diagonal\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# and the hidden state\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m     hidden_states[:,i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,:,:] \u001b[38;5;241m=\u001b[39m \u001b[43mA_bar\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m*\u001b[39mhidden_states[:,i,:,:]\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m    238\u001b[0m         input_transform[:,i,:,:] \u001b[38;5;66;03m# (B,D,N)\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;66;03m# compute outputs in parallel\u001b[39;00m\n\u001b[1;32m    241\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbln,bldn->bld\u001b[39m\u001b[38;5;124m'\u001b[39m, C, hidden_states[:,\u001b[38;5;241m1\u001b[39m:,:,:])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/fx/traceback.py:67\u001b[0m, in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(\u001b[43mtraceback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/traceback.py:232\u001b[0m, in \u001b[0;36mextract_stack\u001b[0;34m(f, limit)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[0;32m--> 232\u001b[0m stack \u001b[38;5;241m=\u001b[39m \u001b[43mStackSummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    233\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/traceback.py:395\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[1;32m    393\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_from_extended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextended_frame_gen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_lines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlookup_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapture_locals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapture_locals\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/traceback.py:434\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[0;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[1;32m    430\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[1;32m    431\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals,\n\u001b[1;32m    432\u001b[0m         end_lineno\u001b[38;5;241m=\u001b[39mend_lineno, colno\u001b[38;5;241m=\u001b[39mcolno, end_colno\u001b[38;5;241m=\u001b[39mend_colno))\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[0;32m--> 434\u001b[0m     \u001b[43mlinecache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/linecache.py:52\u001b[0m, in \u001b[0;36mcheckcache\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     48\u001b[0m         clearcache()\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m---> 52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheckcache\u001b[39m(filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Discard cache entries that are out of date.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    (This is not checked upon each call!)\"\"\"\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training! \n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "train_args = LMTrainingArgs(gpt_3_peak_lr=1.5e-3, warmup_epochs=10, n_epochs=100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "if train_args.optimizer == \"AdamW\":\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                                lr=train_args.peak_lr, \n",
    "                                betas=train_args.adam_beta,\n",
    "                                eps=train_args.adam_epsilon,\n",
    "                                weight_decay=train_args.weight_decay)\n",
    "    \n",
    "elif train_args.optimizer == \"Adam\": # used in synthetic tasks\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                                lr=train_args.peak_lr, \n",
    "                                betas=train_args.adam_beta,\n",
    "                                eps=train_args.adam_epsilon)    \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=train_args.schedule_fn)\n",
    "\n",
    "# train_args.show_lr_schedule()\n",
    "\n",
    "for epoch in range(train_args.n_epochs):\n",
    "    print(f\"Epoch: {epoch + 1}/{train_args.n_epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for in_seq, target_seq in tqdm(train_loader):\n",
    "        out_seq = model(in_seq)\n",
    "        loss = criterion(out_seq.view(-1, mamba_args.vocab_size), target_seq.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), train_args.gradient_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_perplexity = math.exp(train_loss)\n",
    "    print(f\"Train loss: {train_loss:.4f}, Train perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for in_seq, target_seq in val_loader:\n",
    "            out_seq, _ = model(in_seq)\n",
    "            loss = criterion(out_seq.view(-1, mamba_args.vocab_size), target_seq.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_perplexity = math.exp(val_loss)\n",
    "    print(f\"Val loss: {val_loss:.4f}, Val perplexity: {val_perplexity:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
