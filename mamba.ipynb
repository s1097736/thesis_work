{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import jax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat\n",
    "import pickle\n",
    "import collections\n",
    "from tqdm import tqdm \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "@dataclass\n",
    "class MambaArgs:\n",
    "    N: int \n",
    "    D: int\n",
    "    n_layers: int\n",
    "    vocab_size: int\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    expansion_factor: int = 2\n",
    "    conv_1d_size: int = 4\n",
    "    conv_bias: bool = True\n",
    "    general_bias: bool = False # applies to the input and output projections\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.D_inner = int(self.expansion_factor * self.D)\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    ''' Full Mamba architecture '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(Mamba, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.D)\n",
    "\n",
    "        self.layers = nn.ModuleList([ResidualMambaBlock(args) \n",
    "                                     for _ in range(args.n_layers)])\n",
    "        self.norm_f = RMSNorm(args.D)\n",
    "\n",
    "        self.logits = nn.Linear(args.D, args.vocab_size, bias=False)\n",
    "        self.logits.weight = self.embedding.weight # weight tying! \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.logits(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class ResidualMambaBlock(nn.Module):\n",
    "    ''' Wraps the standard Mamba block with RMS normalization and residual\n",
    "        connections (used everywhere)'''\n",
    "    \n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(ResidualMambaBlock, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.block = MambaBlock(args)\n",
    "        self.rms = RMSNorm(args.D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(self.rms(x)) + x\n",
    "    \n",
    "class MambaBlock(nn.Module):\n",
    "    ''' Standard Mamba block as illustrated in the paper '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(MambaBlock, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        # takes care of both of the upscale projections, factor of 2!\n",
    "        self.in_proj = nn.Linear(args.D, 2*args.D_inner, bias=args.general_bias)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.D_inner,\n",
    "            out_channels=args.D_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.conv_1d_size,\n",
    "            groups=args.D_inner,\n",
    "            padding=args.conv_1d_size - 1,\n",
    "        )    \n",
    "        self.s6_block = S6Block(args)    \n",
    "\n",
    "        self.out_proj = nn.Linear(args.D_inner, args.D, bias=args.general_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, _ = x.shape # used to avoid specifying these in args\n",
    "        x = self.in_proj(x)\n",
    "        # split the input into the two paths\n",
    "        (x, res) = x.split(\n",
    "            split_size=[self.args.D_inner, self.args.D_inner], dim=-1)\n",
    "\n",
    "        # input of shape (B,L,D), dimensions need switching for convolution\n",
    "        x = torch.transpose(x, 1,2)\n",
    "        x = self.conv1d(x)[:,:,:l] # the limit is needed because of the padding\n",
    "        x = torch.transpose(x, 1,2)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        x = self.s6_block(x)\n",
    "        x = x * F.silu(res)\n",
    "\n",
    "        y = self.out_proj(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "class S6Block(nn.Module):\n",
    "    ''' Inner SSM block '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "\n",
    "        super(S6Block, self).__init__()\n",
    "        self.args = args \n",
    "\n",
    "        def s4d_real():\n",
    "            # initialization for A used in the paper. Other complex-valued \n",
    "            # initializations also possible\n",
    "\n",
    "            # compute one diagonal, then broadcast across D dimensions\n",
    "            A = -(torch.arange(0,args.N)+1) \n",
    "\n",
    "            return A.unsqueeze(0).repeat(args.D_inner,1).float()\n",
    "\n",
    "        def get_delta_bias():\n",
    "            # sample from a uniform distribution bounded within these values,\n",
    "            # then pass through an inverse softplus\n",
    "            a = 0.001\n",
    "            b = 0.1\n",
    "            sample = (b-a)* torch.rand(1) + a\n",
    "            # no built-in pytorch version of inverse softplus... numerical issues?\n",
    "            return torch.log(torch.exp(sample-1))\n",
    "\n",
    "        self.A = nn.Parameter(s4d_real())\n",
    "        \n",
    "        # these are strictly linear projections, no biases used ever.\n",
    "        # delta uses one, which we manually add later. \n",
    "        self.to_BCdelta = nn.Linear(args.D_inner, 2*args.N+1, bias=False)\n",
    "\n",
    "        # explicit delta bias term\n",
    "        self.delta_bias = nn.Parameter(get_delta_bias()) #TODO:  Check for training\n",
    "        \n",
    "    def discretize(self, delta, B):\n",
    "\n",
    "        # ZOH discretization. Official implementation approximates B_bar with\n",
    "        # Euler step instead\n",
    "        delta_A = torch.einsum('bld,dn->bldn', delta, self.A)\n",
    "        A_bar = torch.exp(delta_A)\n",
    "        delta_B = torch.einsum('bld,bln->bldn', delta, B)\n",
    "        # diagonal matrices, so 1/A is the inverse, subtracting 1 instead \n",
    "        # of the identity matrix, and directly multiplying elementwise for the \n",
    "        # first multiplication (second is defined elementwise anyway)\n",
    "        B_bar = 1/(delta_A) * (A_bar - 1) * delta_B\n",
    "\n",
    "        return A_bar, B_bar\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, _ = x.shape \n",
    "        # generate all projected parameters and split them up\n",
    "        BCdelta = self.to_BCdelta(x)\n",
    "        # delta: (B, L, 1). B, C: (B, L, N)\n",
    "        (B, C, delta) = BCdelta.split(\n",
    "            split_size=[self.args.N, self.args.N, 1], dim=-1)\n",
    "\n",
    "        # broadcasting for delta and computing final parameters\n",
    "        delta = delta.repeat(1,1,self.args.D_inner) # (B,L,D)\n",
    "        delta += self.delta_bias\n",
    "        delta = F.softplus(delta)\n",
    "\n",
    "        # discretization\n",
    "        A_bar, B_bar = self.discretize(delta, B) # (B, L, D, N)\n",
    "        \n",
    "        # input transformation is parallelizable\n",
    "        input_transform = B_bar * x.unsqueeze(-1) # (B, L, D, N)\n",
    "        \n",
    "        # scan through each individual token to compute hidden states\n",
    "        hidden_states = torch.zeros(\n",
    "            b, l+1, self.args.D_inner, self.args.N).to(self.args.device)\n",
    "        \n",
    "        for i in range(0,l):\n",
    "            # because A is represented only through diagonal, Ah_t-1 is \n",
    "            # equivalent to taking the elementwise product of the diagonal\n",
    "            # and the hidden state\n",
    "            hidden_states[:,i+1,:,:] = A_bar[:,i,:,:]*hidden_states[:,i,:,:].clone() + \\\n",
    "                input_transform[:,i,:,:] # (B,D,N)\n",
    "        \n",
    "        # compute outputs in parallel\n",
    "        outputs = torch.einsum('bln,bldn->bld', C, hidden_states[:,1:,:,:])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    ''' Simple implementation of RMSNorm. Default implementation is bugged\n",
    "        in this version of PyTorch, don't want to mess with version updating '''\n",
    "    def __init__(self,\n",
    "                 D: int,\n",
    "                 eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(D))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\n",
      "Archive:  kaggle_song_lyrics_dataset.pkl.zip\n"
     ]
    }
   ],
   "source": [
    "# use a simple word-level tokenizer to create a tokenized version of the kaggle \n",
    "# song lyrics dataset. Simply training to predict the next word here. \n",
    "\n",
    "L = 32\n",
    "B = 8\n",
    "vocab_size = 10000\n",
    "\n",
    "device = 'mps'\n",
    "\n",
    "with open(\"kaggle_song_lyrics_dataset/kaggle_song_lyrics_dataset.pkl\", \"rb\") as f:\n",
    "    seqs = pickle.load(f)\n",
    "\n",
    "vocab       = [\"<unk>\"] + (lambda counter: sorted(counter, key=counter.get, reverse=True))(collections.Counter(seqs))[:vocab_size - 1]\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, device, seq_size, seqs):\n",
    "        super(SeqDataset, self).__init__()\n",
    "        self.device   = device\n",
    "        self.seq_size = seq_size\n",
    "        self.seqs     = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs) - self.seq_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_seq     = torch.tensor(self.seqs[idx    :idx + self.seq_size    ], dtype=torch.long, device=self.device)\n",
    "        target_seq = torch.tensor(self.seqs[idx + 1:idx + self.seq_size + 1], dtype=torch.long, device=self.device)\n",
    "        return in_seq, target_seq\n",
    "\n",
    "seqs = seqs[:len(seqs)//100]\n",
    "\n",
    "train_set = SeqDataset(device, L, [word_to_idx.get(word, 0) for word in seqs[                        :int(0.8 * len(seqs))]])\n",
    "val_set   = SeqDataset(device, L, [word_to_idx.get(word, 0) for word in seqs[int(0.8 * len(seqs)) + 1:int(0.9 * len(seqs))]])\n",
    "test_set  = SeqDataset(device, L, [word_to_idx.get(word, 0) for word in seqs[int(0.9 * len(seqs)) + 1:                    ]])\n",
    "\n",
    "train_loader = DataLoader(train_set, B, True )\n",
    "val_loader   = DataLoader(val_set  , B, False)\n",
    "test_loader  = DataLoader(test_set , B, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1419 [00:17<51:21,  2.18s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m train_loss       \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     31\u001b[0m train_perplexity  \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mexp(train_loss)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/optim/adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    390\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training! \n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "n_epochs = 3\n",
    "\n",
    "D = 16\n",
    "N = 8\n",
    "n_layers = 5\n",
    "args = MambaArgs(N, D, n_layers, vocab_size, device)\n",
    "model = Mamba(args).to(args.device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "min_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch + 1}/{n_epochs}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for in_seq, target_seq in tqdm(train_loader):\n",
    "        out_seq  = model(in_seq)\n",
    "        loss        = criterion(out_seq.view(-1, vocab_size), target_seq.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss       /= len(train_loader)\n",
    "    train_perplexity  = math.exp(train_loss)\n",
    "    print(f\"Train loss: {train_loss:.4f}, Train perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for in_seq, target_seq in val_loader:\n",
    "            out_seq, _  = model(in_seq)\n",
    "            loss        = criterion(out_seq.view(-1, vocab_size), target_seq.view(-1))\n",
    "            val_loss   += loss.item()\n",
    "\n",
    "    val_loss       /= len(val_loader)\n",
    "    val_perplexity  = math.exp(val_loss)\n",
    "    print(f\"Val loss: {val_loss:.4f}, Val perplexity: {val_perplexity:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
