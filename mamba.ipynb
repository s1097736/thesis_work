{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import jax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat\n",
    "import pickle\n",
    "import collections\n",
    "from tqdm import tqdm \n",
    "import math\n",
    "from transformers import GPT2Tokenizer\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "@dataclass\n",
    "class LMTrainingArgs:\n",
    "    # NB this is not the actual learning rate, see below! \n",
    "    gpt_3_peak_lr: float # GPT3 spec, copy from table depending on size\n",
    "    warmup_epochs: int \n",
    "    n_epochs: int\n",
    "\n",
    "    # default arguments, specified according to Mamba paper. This is the \n",
    "    # default for language modeling, for artificial tasks use the other\n",
    "    # args class!\n",
    "\n",
    "    min_lr: float = 1e-5\n",
    "    weight_decay: float = 0.1\n",
    "    gradient_clip: float = 1.0\n",
    "    adam_beta: tuple = (0.9, 0.95)\n",
    "    adam_epsilon: float = 1e-8\n",
    "    optimizer: str = \"AdamW\" # TODO: implement ability to use Adam, just because\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.peak_lr: float = 5*self.gpt_3_peak_lr # the actual lr in the training recipe\n",
    "        self.schedule_fn: Callable[[int], float] = self.lm_learning_schedule\n",
    "\n",
    "        assert self.warmup_epochs < self.n_epochs, \"Warmup epochs > total epochs\"\n",
    "        assert self.optimizer == \"AdamW\" or self.optimizer == \"Adam\", 'Invalid optimizer'\n",
    "        \n",
    "    def lm_learning_schedule(self, epoch):\n",
    "        # a cosine decay with a minimum value, with a linear warm-up\n",
    "        if epoch < self.warmup_epochs:\n",
    "            return float(epoch+1) / float(max(1, self.warmup_epochs))\n",
    "        else:\n",
    "            # calculate amount of decay progress\n",
    "            progress = float(epoch - self.warmup_epochs + 1) / \\\n",
    "                            float(max(1, self.n_epochs - self.warmup_epochs))\n",
    "            # shift cosine function up, rescale, and compute the appropriate amount\n",
    "            # of decay\n",
    "            cosine_decay = 0.5 * (1+ math.cos(math.pi * progress))\n",
    "            # rescale the function again so that it doesn't go below the minimum\n",
    "            # value\n",
    "            return cosine_decay * (1 - self.min_lr / self.peak_lr) + self.min_lr / self.peak_lr\n",
    "    \n",
    "    def show_lr_schedule(self):\n",
    "        # visualization of the learning rate schedule given the specified\n",
    "        # training protocol\n",
    "        epochs = np.arange(0, self.n_epochs)\n",
    "        lr = np.zeros(len(epochs))\n",
    "        for e in epochs:\n",
    "            lr[e] = self.peak_lr*self.lm_learning_schedule(e)\n",
    "\n",
    "        min_lr = np.min(lr[self.warmup_epochs+1:])\n",
    "        max_lr = np.max(lr)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(epochs, lr)\n",
    "        plt.xlim([0,self.n_epochs-1])\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Learning rate\")\n",
    "        plt.title(f\"Max = {max_lr}, \\nMin = {min_lr:.2e}\")\n",
    "        plt.show()\n",
    "\n",
    "@dataclass\n",
    "class MambaArgs:\n",
    "    N: int \n",
    "    D: int\n",
    "    n_layers: int\n",
    "    vocab_size: int\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    expansion_factor: int = 2\n",
    "    conv_1d_size: int = 4\n",
    "    conv_bias: bool = True\n",
    "    general_bias: bool = False # applies to the input and output projections\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.D_inner = int(self.expansion_factor * self.D)\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    ''' Full Mamba architecture '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(Mamba, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.D)\n",
    "\n",
    "        self.layers = nn.ModuleList([ResidualMambaBlock(args) \n",
    "                                     for _ in range(args.n_layers)])\n",
    "        self.norm_f = RMSNorm(args.D)\n",
    "\n",
    "        self.logits = nn.Linear(args.D, args.vocab_size, bias=False)\n",
    "        self.logits.weight = self.embedding.weight # weight tying! \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.logits(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class ResidualMambaBlock(nn.Module):\n",
    "    ''' Wraps the standard Mamba block with RMS normalization and residual\n",
    "        connections (used everywhere)'''\n",
    "    \n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(ResidualMambaBlock, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.block = MambaBlock(args)\n",
    "        self.rms = RMSNorm(args.D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(self.rms(x)) + x\n",
    "    \n",
    "class MambaBlock(nn.Module):\n",
    "    ''' Standard Mamba block as illustrated in the paper '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(MambaBlock, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        # takes care of both of the upscale projections, factor of 2!\n",
    "        self.in_proj = nn.Linear(args.D, 2*args.D_inner, bias=args.general_bias)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.D_inner,\n",
    "            out_channels=args.D_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.conv_1d_size,\n",
    "            groups=args.D_inner,\n",
    "            padding=args.conv_1d_size - 1,\n",
    "        )    \n",
    "        self.s6_block = S6Block(args)    \n",
    "\n",
    "        self.out_proj = nn.Linear(args.D_inner, args.D, bias=args.general_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, _ = x.shape # used to avoid specifying these in args\n",
    "        x = self.in_proj(x)\n",
    "        # split the input into the two paths\n",
    "        (x, res) = x.split(\n",
    "            split_size=[self.args.D_inner, self.args.D_inner], dim=-1)\n",
    "\n",
    "        # input of shape (B,L,D), dimensions need switching for convolution\n",
    "        x = torch.transpose(x, 1,2)\n",
    "        x = self.conv1d(x)[:,:,:l] # the limit is needed because of the padding\n",
    "        x = torch.transpose(x, 1,2)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        x = self.s6_block(x)\n",
    "        x = x * F.silu(res)\n",
    "\n",
    "        y = self.out_proj(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "class S6Block(nn.Module):\n",
    "    ''' Inner SSM block '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "\n",
    "        super(S6Block, self).__init__()\n",
    "        self.args = args \n",
    "\n",
    "        def s4d_real():\n",
    "            # initialization for A used in the paper. Other complex-valued \n",
    "            # initializations also possible\n",
    "\n",
    "            # compute one diagonal, then broadcast across D dimensions\n",
    "            A = -(torch.arange(0,args.N)+1) \n",
    "\n",
    "            return A.unsqueeze(0).repeat(args.D_inner,1).float()\n",
    "\n",
    "        def get_delta_bias():\n",
    "            # sample from a uniform distribution bounded within these values,\n",
    "            # then pass through an inverse softplus\n",
    "            a = 0.001\n",
    "            b = 0.1\n",
    "            sample = (b-a)* torch.rand(1) + a\n",
    "            # no built-in pytorch version of inverse softplus... numerical issues?\n",
    "            return torch.log(torch.exp(sample-1))\n",
    "\n",
    "        self.A = nn.Parameter(s4d_real())\n",
    "        \n",
    "        # these are strictly linear projections, no biases used ever.\n",
    "        # delta uses one, which we manually add later. \n",
    "        self.to_BCdelta = nn.Linear(args.D_inner, 2*args.N+1, bias=False)\n",
    "\n",
    "        # explicit delta bias term\n",
    "        self.delta_bias = nn.Parameter(get_delta_bias()) #TODO:  Check for training\n",
    "        \n",
    "    def discretize(self, delta, B):\n",
    "\n",
    "        # ZOH discretization. Official implementation approximates B_bar with\n",
    "        # Euler step instead\n",
    "        delta_A = torch.einsum('bld,dn->bldn', delta, self.A)\n",
    "        A_bar = torch.exp(delta_A)\n",
    "        delta_B = torch.einsum('bld,bln->bldn', delta, B)\n",
    "        # diagonal matrices, so 1/A is the inverse, subtracting 1 instead \n",
    "        # of the identity matrix, and directly multiplying elementwise for the \n",
    "        # first multiplication (second is defined elementwise anyway)\n",
    "        B_bar = 1/(delta_A) * (A_bar - 1) * delta_B\n",
    "\n",
    "        return A_bar, B_bar\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, _ = x.shape \n",
    "        # generate all projected parameters and split them up\n",
    "        BCdelta = self.to_BCdelta(x)\n",
    "        # delta: (B, L, 1). B, C: (B, L, N)\n",
    "        (B, C, delta) = BCdelta.split(\n",
    "            split_size=[self.args.N, self.args.N, 1], dim=-1)\n",
    "\n",
    "        # broadcasting for delta and computing final parameters\n",
    "        delta = delta.repeat(1,1,self.args.D_inner) # (B,L,D)\n",
    "        delta += self.delta_bias\n",
    "        delta = F.softplus(delta)\n",
    "\n",
    "        # discretization\n",
    "        A_bar, B_bar = self.discretize(delta, B) # (B, L, D, N)\n",
    "        \n",
    "        # input transformation is parallelizable\n",
    "        input_transform = B_bar * x.unsqueeze(-1) # (B, L, D, N)\n",
    "        \n",
    "        # scan through each individual token to compute hidden states\n",
    "        hidden_states = torch.zeros(\n",
    "            b, l+1, self.args.D_inner, self.args.N).to(self.args.device)\n",
    "        \n",
    "        for i in range(0,l):\n",
    "            # because A is represented only through diagonal, Ah_t-1 is \n",
    "            # equivalent to taking the elementwise product of the diagonal\n",
    "            # and the hidden state\n",
    "            hidden_states[:,i+1,:,:] = A_bar[:,i,:,:]*hidden_states[:,i,:,:].clone() + \\\n",
    "                input_transform[:,i,:,:] # (B,D,N)\n",
    "        \n",
    "        # compute outputs in parallel\n",
    "        outputs = torch.einsum('bln,bldn->bld', C, hidden_states[:,1:,:,:])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    ''' Simple implementation of RMSNorm. Default implementation is bugged\n",
    "        in this version of PyTorch, don't want to mess with version updating '''\n",
    "    def __init__(self,\n",
    "                 D: int,\n",
    "                 eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(D))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use GPT3 tokenizer tokenizer to create a tokenized version of the kaggle \n",
    "# song lyrics dataset. Simply training to predict the next word here. \n",
    "\n",
    "L = 32\n",
    "B = 8\n",
    "D = 16\n",
    "N = 8\n",
    "\n",
    "vocab_size = 10000\n",
    "device = 'mps' # hehe \n",
    "\n",
    "mamba_args = MambaArgs(N, D, n_layers=5, vocab_size=vocab_size, device=device)\n",
    "model = Mamba(mamba_args).to(mamba_args.device)\n",
    "\n",
    "# tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", force_download=True)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"model\")\n",
    "\n",
    "with open(\"kaggle_song_lyrics_dataset/kaggle_song_lyrics_dataset.pkl\", \"rb\") as f:\n",
    "    seqs = pickle.load(f)\n",
    "\n",
    "# get tokens and corresponding IDs\n",
    "tokens = [tokenizer.tokenize(word) for word in seqs]\n",
    "token_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens]\n",
    "\n",
    "# flattening them all in one list\n",
    "token_ids = [item for sublist in token_ids for item in sublist] \n",
    "\n",
    "# limit all tokens to the top n most common. Replace the less common occurrences\n",
    "# with unk token \n",
    "filtered_ids = []\n",
    "unk_id = tokenizer.get_vocab()['unk']\n",
    "for token_id in token_ids:\n",
    "\n",
    "    # if given vocab size is not large enough to include the unk token itself, \n",
    "    # the vocab size must be reduced by 1 to fit this in\n",
    "    if unk_id > mamba_args.vocab_size-1:\n",
    "        vocab_limit = mamba_args.vocab_size-2\n",
    "    else:\n",
    "        vocab_limit = mamba_args.vocab_size-1\n",
    "\n",
    "    # filter the text to only include top n most common words\n",
    "    if token_id > vocab_limit and token_id != unk_id:\n",
    "        filtered_ids.append(unk_id)\n",
    "    else:\n",
    "        filtered_ids.append(token_id)\n",
    "\n",
    "# put the tokenized sequences in datasets + dataloaders\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, device, seq_size, seqs):\n",
    "        super(SeqDataset, self).__init__()\n",
    "        self.device   = device\n",
    "        self.seq_size = seq_size\n",
    "        self.seqs     = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs) - self.seq_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_seq     = torch.tensor(self.seqs[idx    :idx + self.seq_size    ], dtype=torch.long, device=self.device)\n",
    "        target_seq = torch.tensor(self.seqs[idx + 1:idx + self.seq_size + 1], dtype=torch.long, device=self.device)\n",
    "        return in_seq, target_seq\n",
    "\n",
    "# 90 10 10 split between the 3 datasets\n",
    "train_set = SeqDataset(mamba_args.device, L, filtered_ids[:int(0.8 * len(filtered_ids))])\n",
    "val_set   = SeqDataset(mamba_args.device, L, filtered_ids[ int(0.8 * len(filtered_ids)) + 1:int(0.9 * len(filtered_ids))])\n",
    "test_set  = SeqDataset(mamba_args.device, L, filtered_ids[ int(0.9 * len(filtered_ids)) + 1:])\n",
    "\n",
    "train_loader = DataLoader(train_set, B, shuffle=True)\n",
    "val_loader   = DataLoader(val_set  , B, shuffle=False)\n",
    "test_loader  = DataLoader(test_set , B, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/167610 [00:05<132:21:14,  2.84s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[199], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     27\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# gradient clipping\u001b[39;00m\n\u001b[1;32m     31\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), train_args\u001b[38;5;241m.\u001b[39mgradient_clip)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training! \n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "train_args = LMTrainingArgs(gpt_3_peak_lr=1.5e-3, warmup_epochs=10, n_epochs=100)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), \n",
    "                              lr=train_args.peak_lr, \n",
    "                              betas=train_args.adam_beta,\n",
    "                              eps=train_args.adam_epsilon,\n",
    "                              weight_decay=train_args.weight_decay)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=train_args.schedule_fn)\n",
    "\n",
    "# train_args.show_lr_schedule()\n",
    "\n",
    "for epoch in range(train_args.n_epochs):\n",
    "    print(f\"Epoch: {epoch + 1}/{train_args.n_epochs}\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for in_seq, target_seq in tqdm(train_loader):\n",
    "        out_seq = model(in_seq)\n",
    "        loss = criterion(out_seq.view(-1, mamba_args.vocab_size), target_seq.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), train_args.gradient_clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "    train_perplexity = math.exp(train_loss)\n",
    "    print(f\"Train loss: {train_loss:.4f}, Train perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for in_seq, target_seq in val_loader:\n",
    "            out_seq, _ = model(in_seq)\n",
    "            loss = criterion(out_seq.view(-1, mamba_args.vocab_size), target_seq.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_perplexity = math.exp(val_loss)\n",
    "    print(f\"Val loss: {val_loss:.4f}, Val perplexity: {val_perplexity:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
