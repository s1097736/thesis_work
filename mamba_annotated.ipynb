{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import jax\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from einops import rearrange, repeat\n",
    "import pickle\n",
    "import collections\n",
    "from tqdm import tqdm \n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "@dataclass\n",
    "class MambaArgs:\n",
    "    N: int \n",
    "    D: int\n",
    "    n_layers: int\n",
    "    vocab_size: int\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    expansion_factor: int = 2\n",
    "    conv_1d_size: int = 4\n",
    "    conv_bias: bool = True\n",
    "    general_bias: bool = False # applies to the input and output projections\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.D_inner = int(self.expansion_factor * self.D)\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    ''' Full Mamba architecture '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(Mamba, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        self.embedding = nn.Embedding(args.vocab_size, args.D)\n",
    "\n",
    "        self.layers = nn.ModuleList([ResidualMambaBlock(args) \n",
    "                                     for _ in range(args.n_layers)])\n",
    "        self.norm_f = RMSNorm(args.D)\n",
    "\n",
    "        self.logits = nn.Linear(args.D, args.vocab_size, bias=False)\n",
    "        self.logits.weight = self.embedding.weight # weight tying! \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        x = self.norm_f(x)\n",
    "        logits = self.logits(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "class ResidualMambaBlock(nn.Module):\n",
    "    ''' Wraps the standard Mamba block with RMS normalization and residual\n",
    "        connections (used everywhere)'''\n",
    "    \n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(ResidualMambaBlock, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "        self.block = MambaBlock(args)\n",
    "        self.rms = RMSNorm(args.D)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.block(self.rms(x)) + x\n",
    "    \n",
    "class MambaBlock(nn.Module):\n",
    "    ''' Standard Mamba block as illustrated in the paper '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "        super(MambaBlock, self).__init__()\n",
    "\n",
    "        self.args = args\n",
    "\n",
    "        # takes care of both of the upscale projections, factor of 2!\n",
    "        self.in_proj = nn.Linear(args.D, 2*args.D_inner, bias=args.general_bias)\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.D_inner,\n",
    "            out_channels=args.D_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.conv_1d_size,\n",
    "            groups=args.D_inner,\n",
    "            padding=args.conv_1d_size - 1,\n",
    "        )    \n",
    "        self.s6_block = S6Block(args)    \n",
    "\n",
    "        self.out_proj = nn.Linear(args.D_inner, args.D, bias=args.general_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, _ = x.shape # used to avoid specifying these in args\n",
    "        x = self.in_proj(x)\n",
    "        # split the input into the two paths\n",
    "        (x, res) = x.split(\n",
    "            split_size=[self.args.D_inner, self.args.D_inner], dim=-1)\n",
    "\n",
    "        # input of shape (B,L,D), dimensions need switching for convolution\n",
    "        x = torch.transpose(x, 1,2)\n",
    "        x = self.conv1d(x)[:,:,:l] # the limit is needed because of the padding\n",
    "        x = torch.transpose(x, 1,2)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        x = self.s6_block(x)\n",
    "        x = x * F.silu(res)\n",
    "\n",
    "        y = self.out_proj(x)\n",
    "\n",
    "        return y\n",
    "\n",
    "class S6Block(nn.Module):\n",
    "    ''' Inner SSM block '''\n",
    "    def __init__(self, args: MambaArgs):\n",
    "\n",
    "        super(S6Block, self).__init__()\n",
    "        self.args = args \n",
    "\n",
    "        def s4d_real():\n",
    "            # initialization for A used in the paper. Other complex-valued \n",
    "            # initializations also possible\n",
    "\n",
    "            # compute one diagonal, then broadcast across D dimensions\n",
    "            A = -(torch.arange(0,args.N)+1) \n",
    "\n",
    "            return A.unsqueeze(0).repeat(args.D_inner,1).float()\n",
    "\n",
    "        def get_delta_bias():\n",
    "            # sample from a uniform distribution bounded within these values,\n",
    "            # then pass through an inverse softplus\n",
    "            a = 0.001\n",
    "            b = 0.1\n",
    "            sample = (b-a)* torch.rand(1) + a\n",
    "            # no built-in pytorch version of inverse softplus... numerical issues?\n",
    "            return torch.log(torch.exp(sample-1))\n",
    "\n",
    "        # the same A is broadcasted across all D token dimensions. A is a \n",
    "        # diagonal matrix, which is why we represent it only with its diagonal\n",
    "        # elements. \n",
    "        self.A = nn.Parameter(s4d_real())\n",
    "        \n",
    "        # B, C, and delta are different for each token, but the linear projections\n",
    "        # which generate them are shared across all tokens and batches. All\n",
    "        # parameters are generated at once, and then split + broadcasted\n",
    "        # as necessary. These are strictly linear projections, no biases used.\n",
    "        # Delta uses one, which we manually add later. \n",
    "        self.to_BCdelta = nn.Linear(args.D_inner, 2*args.N+1, bias=False)\n",
    "\n",
    "        # explicit delta bias term. Same term used for each projection across\n",
    "        # batches and sequences. \n",
    "        self.delta_bias = nn.Parameter(get_delta_bias())\n",
    "        \n",
    "    def discretize(self, delta, B):\n",
    "\n",
    "        delta_A = torch.einsum('bld,dn->bldn', delta, self.A)\n",
    "        # effect: multiplying each vector N to be used at every l, b by the \n",
    "        # corresponding discretization constant at that position, then broadcasted \n",
    "        # over a new dimension D (we want to use the same A matrix for each input dimension)\n",
    "        A_bar = torch.exp(delta_A)\n",
    "        delta_B = torch.einsum('bld,bln->bldn', delta, B)\n",
    "        # same effect as for delta_A, but shapes are different because B is \n",
    "        # directly defined for each l, b\n",
    "        B_bar = 1/(delta_A) * (A_bar - 1) * delta_B\n",
    "        # diagonal matrices, so 1/A is the inverse, subtracting 1 instead \n",
    "        # of the identity matrix, and directly multiplying elementwise for the \n",
    "        # first multiplication (second is defined elementwise anyway)\n",
    "\n",
    "        return A_bar, B_bar\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, l, _ = x.shape \n",
    "        # generate all projected parameters and split them up\n",
    "        BCdelta = self.to_BCdelta(x)\n",
    "        # delta: (B, L, 1). B, C: (B, L, N)\n",
    "        (B, C, delta) = BCdelta.split(\n",
    "            split_size=[self.args.N, self.args.N, 1], dim=-1)\n",
    "\n",
    "        # broadcasting for delta and computing final parameters\n",
    "        delta = delta.repeat(1,1,self.args.D_inner) # (B,L,D)\n",
    "        delta += self.delta_bias\n",
    "        delta = F.softplus(delta)\n",
    "\n",
    "        # discretization\n",
    "        A_bar, B_bar = self.discretize(delta, B) # (B, L, D, N)\n",
    "        \n",
    "        # input transformation is parallelizable\n",
    "        input_transform = B_bar * x.unsqueeze(-1) # (B, L, D, N)\n",
    "        \n",
    "        # scan through each individual token to compute hidden states\n",
    "        hidden_states = torch.zeros(\n",
    "            b, l+1, self.args.D_inner, self.args.N).to(self.args.device)\n",
    "        \n",
    "        for i in range(0,l):\n",
    "            # because A is represented only through diagonal, Ah_t-1 is \n",
    "            # equivalent to taking the elementwise product of the diagonal\n",
    "            # and the hidden state\n",
    "            hidden_states[:,i+1,:,:] = A_bar[:,i,:,:]*hidden_states[:,i,:,:].clone() + \\\n",
    "                input_transform[:,i,:,:] # (B,D,N)\n",
    "        \n",
    "        # compute outputs in parallel\n",
    "        outputs = torch.einsum('bln,bldn->bld', C, hidden_states[:,1:,:,:])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    ''' Simple implementation of RMSNorm. Default implementation is bugged\n",
    "        in this version of PyTorch, don't want to mess with version updating '''\n",
    "    def __init__(self,\n",
    "                 D: int,\n",
    "                 eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(D))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a simple word-level tokenizer to create a tokenized version of the kaggle \n",
    "# song lyrics dataset. Simply training to predict the next word here. \n",
    "\n",
    "L = 32\n",
    "B = 8\n",
    "vocab_size = 10000\n",
    "\n",
    "# device = 'mps'\n",
    "\n",
    "!wget -nc https://umuguc.github.io/file-sharing/kaggle_song_lyrics_dataset.pkl.zip\n",
    "!unzip -n kaggle_song_lyrics_dataset.pkl.zip -d kaggle_song_lyrics_dataset\n",
    "\n",
    "with open(\"kaggle_song_lyrics_dataset/kaggle_song_lyrics_dataset.pkl\", \"rb\") as f:\n",
    "    seqs = pickle.load(f)\n",
    "\n",
    "vocab       = [\"<unk>\"] + (lambda counter: sorted(counter, key=counter.get, reverse=True))(collections.Counter(seqs))[:vocab_size - 1]\n",
    "idx_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, device, seq_size, seqs):\n",
    "        super(SeqDataset, self).__init__()\n",
    "        self.device   = device\n",
    "        self.seq_size = seq_size\n",
    "        self.seqs     = seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.seqs) - self.seq_size - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        in_seq     = torch.tensor(self.seqs[idx    :idx + self.seq_size    ], dtype=torch.long, device=self.device)\n",
    "        target_seq = torch.tensor(self.seqs[idx + 1:idx + self.seq_size + 1], dtype=torch.long, device=self.device)\n",
    "        return in_seq, target_seq\n",
    "\n",
    "seqs = seqs[:len(seqs)//100]\n",
    "\n",
    "train_set = SeqDataset(device, L, [word_to_idx.get(word, 0) for word in seqs[                        :int(0.8 * len(seqs))]])\n",
    "val_set   = SeqDataset(device, L, [word_to_idx.get(word, 0) for word in seqs[int(0.8 * len(seqs)) + 1:int(0.9 * len(seqs))]])\n",
    "test_set  = SeqDataset(device, L, [word_to_idx.get(word, 0) for word in seqs[int(0.9 * len(seqs)) + 1:                    ]])\n",
    "\n",
    "train_loader = DataLoader(train_set, B, True )\n",
    "val_loader   = DataLoader(val_set  , B, False)\n",
    "test_loader  = DataLoader(test_set , B, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 6/1419 [00:12<48:33,  2.06s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 27\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m train_loss       \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/thesis_env/lib/python3.12/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training! \n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "n_epochs = 3\n",
    "\n",
    "D = 16\n",
    "N = 8\n",
    "n_layers = 5\n",
    "args = MambaArgs(N, D, n_layers, vocab_size, device)\n",
    "model = Mamba(args).to(args.device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "min_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch: {epoch + 1}/{n_epochs}\")\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for in_seq, target_seq in tqdm(train_loader):\n",
    "        out_seq  = model(in_seq)\n",
    "        loss        = criterion(out_seq.view(-1, vocab_size), target_seq.view(-1))\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss       /= len(train_loader)\n",
    "    train_perplexity  = math.exp(train_loss)\n",
    "    print(f\"Train loss: {train_loss:.4f}, Train perplexity: {train_perplexity:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for in_seq, target_seq in val_loader:\n",
    "            out_seq, _  = model(in_seq)\n",
    "            loss        = criterion(out_seq.view(-1, vocab_size), target_seq.view(-1))\n",
    "            val_loss   += loss.item()\n",
    "\n",
    "    val_loss       /= len(val_loader)\n",
    "    val_perplexity  = math.exp(val_loss)\n",
    "    print(f\"Val loss: {val_loss:.4f}, Val perplexity: {val_perplexity:.4f}\")\n",
    "    \n",
    "    # if val_loss < min_loss:\n",
    "    #     min_loss = val_loss\n",
    "    #     torch.save(model.state_dict(), f\"kaggle_song_lyrics_dataset/{MODEL_NAME:s}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
